# Phase 1 개발 일지 - 인프라 구축

**작성일**: 2025-10-01  
**Phase**: Phase 1 - 인프라 구축  
**작업 시간**: 약 2시간  
**작성자**: 개발팀 + Claude

---

## 개요

Phase 1의 목표는 CS AI 에이전트 프로젝트를 개발하기 위한 기본 인프라를 완전히 구축하는 것이었습니다. 이것은 단순히 소프트웨어를 설치하는 것을 넘어서, 네 개의 서로 다른 데이터베이스 서비스가 Docker 컨테이너 안에서 실행되고, Python 환경이 GPU를 활용할 수 있으며, 모든 패키지가 서로 호환되는 완전한 개발 환경을 만드는 작업이었습니다.

이 작업은 제로 베이스에서 시작했기 때문에, Python에 대한 사전 지식이 없는 상태에서 진행되었습니다. 따라서 각 단계마다 개념 설명과 함께 진행되었고, 발생한 문제들도 하나씩 해결해나갔습니다.

---

## 작업 환경

### 하드웨어 사양
- **OS**: Windows 11 Pro 64bit
- **CPU**: AMD Ryzen 5 9600 (6-Core, 3.80 GHz)
- **RAM**: 64GB
- **GPU**: NVIDIA GeForce RTX 3050 (6GB VRAM)

### 목표 구성
- **컨테이너 플랫폼**: Docker Desktop with WSL 2
- **Python 버전**: 3.11.9
- **주요 서비스**:
  - Weaviate 1.25.5 (벡터 데이터베이스)
  - MongoDB 7.0 (NoSQL 데이터베이스)
  - Redis 7.2 (캐싱)
  - Mongo Express 1.0 (MongoDB 관리 도구)

---

## 작업 진행 과정

### 1단계: WSL 2 설치 및 설정

**목적**: Docker Desktop이 Windows에서 작동하기 위해서는 WSL 2가 필수적입니다. WSL은 Windows Subsystem for Linux의 약자로, Windows 환경에서 리눅스를 실행할 수 있게 해주는 기술입니다. Docker 컨테이너는 본질적으로 리눅스 기반이기 때문에, Windows에서 Docker를 사용하려면 WSL 2가 필요합니다.

**진행 과정**:

PowerShell을 관리자 권한으로 실행한 후 다음 명령어를 입력했습니다.

```powershell
wsl --version
```

처음에는 WSL이 설치되어 있지 않다는 메시지가 나타났고, 자동으로 설치 프로세스가 시작되었습니다. 이 과정에서 WSL 2.6.1 버전과 커널 6.6.87이 설치되었습니다. 특히 중요한 것은 VirtualMachinePlatform이라는 Windows 기능이 활성화된 점입니다. 이것은 Windows의 하이퍼바이저 기능으로, 가상 머신을 실행할 수 있게 해주는 커널 수준의 기능입니다.

설치가 완료된 후 시스템 재부팅을 진행했습니다. 재부팅이 필요한 이유는 VirtualMachinePlatform 같은 커널 수준의 기능은 Windows가 부팅될 때 초기화되기 때문입니다. 재부팅 후 다시 `wsl --version` 명령어로 확인했을 때, WSL이 정상적으로 설치되었음을 확인했습니다.

**결과**:
```
WSL 버전: 2.6.1.0
커널 버전: 6.6.87.2-1
WSLg 버전: 1.0.66
```

**교훈**: WSL 설치 시 재부팅을 하지 않아도 일부 명령어는 작동할 수 있지만, 완전한 기능을 위해서는 재부팅이 필수적입니다. 특히 Docker Desktop을 설치할 때 문제가 발생하지 않으려면 재부팅을 먼저 해야 합니다.

---

### 2단계: Docker Desktop 설치

**목적**: Docker Desktop은 Windows에서 컨테이너를 쉽게 관리할 수 있게 해주는 GUI 애플리케이션입니다. 우리 프로젝트에서는 Weaviate, MongoDB, Redis라는 세 개의 서로 다른 데이터베이스를 동시에 실행해야 하는데, Docker를 사용하면 이들을 독립적인 컨테이너로 실행하면서도 서로 네트워크로 연결할 수 있습니다.

**진행 과정**:

공식 웹사이트(https://www.docker.com/products/docker-desktop/)에서 Windows용 설치 파일을 다운로드했습니다. 설치 과정에서 가장 중요한 옵션은 "Use WSL 2 instead of Hyper-V"였습니다. 이 옵션을 선택하면 Docker가 WSL 2를 백엔드로 사용하게 되어, Windows와 리눅스 간의 통합이 훨씬 부드럽게 이루어집니다.

설치가 완료된 후 Docker Desktop을 실행했습니다. 처음 실행할 때는 서비스 약관에 동의하고 추천 설정을 선택하는 과정을 거쳤습니다. Docker Desktop의 하단에 있는 아이콘이 초록색으로 변하면 Docker가 정상적으로 실행되고 있다는 의미입니다.

설치가 제대로 되었는지 확인하기 위해 PowerShell에서 다음 명령어들을 실행했습니다.

```bash
docker --version
docker-compose --version
```

두 명령어 모두 버전 정보를 정상적으로 출력했고, 이것은 Docker가 완벽하게 설치되었다는 것을 의미합니다.

**결과**: Docker Desktop이 정상 작동하며, docker-compose 명령어도 사용 가능한 상태가 되었습니다.

---

### 3단계: Docker Compose로 서비스 실행

**목적**: 프로젝트에 필요한 네 개의 서비스(Weaviate, MongoDB, Redis, Mongo Express)를 한 번에 실행하고 관리하기 위해 docker-compose를 사용합니다. 이미 Phase 0에서 docker-compose.yml 파일을 작성해두었기 때문에, 이 파일을 실행하기만 하면 됩니다.

**진행 과정**:

프로젝트의 docker 폴더로 이동한 후 다음 명령어를 실행했습니다.

```bash
cd C:\workspace\CSAI\docker
docker-compose up -d
```

`-d` 옵션은 detached 모드를 의미하며, 컨테이너들이 백그라운드에서 실행되도록 합니다. 명령어를 실행하자 Docker는 필요한 이미지들을 인터넷에서 다운로드하기 시작했습니다. 처음 실행할 때는 이미지 다운로드에 약 2-3분이 소요되었습니다.

모든 컨테이너가 시작된 후 상태를 확인하기 위해 다음 명령어를 실행했습니다.

```bash
docker-compose ps
```

결과적으로 네 개의 컨테이너 모두 "Up" 상태로 정상 실행되고 있음을 확인했습니다.

**서비스별 검증**:

각 서비스가 실제로 작동하는지 확인하기 위해 여러 방법을 사용했습니다.

첫 번째로 Weaviate를 검증했습니다. 브라우저에서 http://localhost:8080/v1/.well-known/ready 에 접속했을 때, 처음에는 404 에러가 발생했습니다. 이것은 혼란스러웠지만, 로그를 확인한 결과 Weaviate가 정상적으로 시작되었음을 알 수 있었습니다. 다시 http://localhost:8080/v1 에 접속하자 API 엔드포인트 목록이 JSON 형식으로 반환되었습니다. 404 에러는 사실 브라우저가 자동으로 요청하는 favicon.ico에 대한 것이었습니다.

PowerShell에서 Invoke-WebRequest를 사용해 직접 API를 호출해보니, HTTP 200 상태 코드가 반환되었습니다. Weaviate 1.25.5 버전에서는 ready 엔드포인트가 텍스트 대신 HTTP 상태 코드만으로 준비 상태를 표현하도록 변경되었다는 것을 알게 되었습니다.

두 번째로 MongoDB를 검증했습니다. Mongo Express(http://localhost:8081)에 접속하자 웹 기반 관리 인터페이스가 정상적으로 표시되었습니다. 아직 데이터를 넣지 않았기 때문에 데이터베이스 목록은 비어있었지만, 이것은 정상적인 상태입니다.

세 번째로 Redis는 백그라운드 서비스이기 때문에 별도의 UI가 없지만, docker-compose ps 명령어에서 "Up" 상태로 표시되었고, 이것으로 정상 작동을 확인할 수 있었습니다.

**결과**: 네 개의 서비스 모두 정상적으로 실행되고, 외부에서 접근 가능한 상태가 되었습니다.

---

### 4단계: Python 환경 설정

**목적**: Python 가상환경을 만들고 프로젝트에 필요한 모든 패키지를 설치합니다. 가상환경을 사용하는 이유는 프로젝트마다 서로 다른 버전의 라이브러리가 필요할 수 있기 때문입니다. 가상환경을 사용하면 각 프로젝트가 독립적인 패키지 공간을 가지게 되어, 한 프로젝트에서 패키지를 업데이트해도 다른 프로젝트에 영향을 주지 않습니다.

**진행 과정**:

먼저 Python 3.11.9를 공식 웹사이트에서 다운로드하고 설치했습니다. 설치 시 "Add Python to PATH" 옵션을 체크하는 것이 중요합니다. 이 옵션을 선택해야 어디서든 python 명령어를 실행할 수 있기 때문입니다.

backend 폴더로 이동한 후 가상환경을 생성했습니다.

```bash
cd C:\workspace\CSAI\backend
python -m venv venv
```

이 명령어는 venv라는 이름의 가상환경을 만듭니다. 생성된 가상환경을 활성화하기 위해 다음 명령어를 실행했습니다.

```bash
venv\Scripts\activate
```

가상환경이 활성화되면 프롬프트 앞에 (venv)라는 표시가 나타납니다. 이것은 현재 가상환경 안에서 작업하고 있다는 시각적인 표시입니다.

다음으로 requirements.txt에 명시된 모든 패키지를 설치했습니다.

```bash
pip install -r requirements.txt
```

이 과정에서 총 20개 이상의 패키지가 설치되었습니다. FastAPI, spaCy, Sentence-Transformers, PyTorch, Weaviate client, MongoDB client, Redis client, Claude API client 등이 포함되어 있습니다. 설치는 약 5-10분 정도 소요되었습니다.

spaCy의 경우 패키지 설치 후 별도로 한국어 모델을 다운로드해야 합니다. 다음 명령어로 대형 한국어 모델을 다운로드했습니다.

```bash
python -m spacy download ko_core_news_lg
```

이 모델은 약 500MB 크기로, 수백만 개의 한국어 단어와 문법 규칙, 개체명 인식 패턴이 포함되어 있습니다.

**패키지 검증**:

모든 패키지가 제대로 설치되었는지 확인하기 위해 import 테스트를 진행했습니다.

```bash
python -c "import weaviate; print('Weaviate:', weaviate.__version__)"
python -c "import fastapi; print('FastAPI:', fastapi.__version__)"
python -c "import anthropic; print('Anthropic:', anthropic.__version__)"
python -c "import torch; print('PyTorch:', torch.__version__)"
python -c "import spacy; print('spaCy:', spacy.__version__)"
```

모든 패키지가 정상적으로 import되었고 버전 정보도 출력되었습니다.

**결과**: Python 가상환경이 구성되었고, 프로젝트에 필요한 모든 패키지가 설치되어 사용 가능한 상태가 되었습니다.

---

### 5단계: GPU 환경 설정

**목적**: 프로젝트에서는 Sentence-BERT를 사용해 텍스트 임베딩을 생성하게 됩니다. 이 작업은 CPU로도 가능하지만, GPU를 사용하면 수십 배에서 수백 배 빠르게 처리할 수 있습니다. RTX 3050 GPU를 활용하기 위해서는 PyTorch가 CUDA를 지원하는 버전으로 설치되어야 합니다.

**진행 과정**:

먼저 GPU 인식 여부를 확인하기 위해 다음 명령어를 실행했습니다.

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

결과는 "CUDA available: False"였습니다. 이것은 PyTorch가 CPU 전용 버전으로 설치되었다는 의미입니다. pip install을 할 때 기본적으로는 CPU 버전이 설치되기 때문입니다.

NVIDIA 드라이버가 제대로 설치되어 있는지 확인하기 위해 nvidia-smi 명령어를 실행했습니다.

```bash
nvidia-smi
```

출력 결과는 다음과 같았습니다.

```
Driver Version: 560.94
CUDA Version: 12.6
GPU Name: NVIDIA GeForce RTX 3050
```

드라이버는 정상적으로 설치되어 있었고, CUDA 12.6을 지원하는 것으로 나타났습니다. 따라서 문제는 PyTorch 설치 방식에 있었습니다.

PyTorch를 CUDA 버전으로 재설치하기 위해 먼저 기존 버전을 제거했습니다.

```bash
pip uninstall torch torchvision torchaudio -y
```

그리고 CUDA 12.1 버전의 PyTorch를 설치했습니다. PyTorch 2.4.1은 CUDA 12.1로 빌드되어 있지만, CUDA는 하위 호환성을 지원하기 때문에 CUDA 12.6 드라이버에서도 완벽하게 작동합니다.

```bash
pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

여기서 중요한 것은 `--index-url` 옵션입니다. 이 옵션은 pip에게 기본 PyPI가 아닌 PyTorch의 특별한 저장소에서 패키지를 다운로드하라고 알려줍니다. 이 저장소에는 CUDA 지원이 포함된 PyTorch 바이너리가 저장되어 있습니다.

재설치 후 다시 GPU 인식 여부를 확인했습니다.

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('Device name:', torch.cuda.get_device_name(0))"
```

이번에는 다음과 같은 결과가 나왔습니다.

```
CUDA available: True
Device name: NVIDIA GeForce RTX 3050
```

**결과**: PyTorch가 RTX 3050 GPU를 정상적으로 인식하고 있으며, 이제 딥러닝 연산을 GPU에서 수행할 수 있습니다.

**교훈**: requirements.txt에는 torch 버전만 명시하고 CUDA 버전은 명시하지 않는 것이 올바른 접근 방식입니다. 각 개발자나 서버마다 다른 GPU를 사용할 수 있고, 어떤 환경에서는 GPU가 없을 수도 있기 때문입니다. 대신 설치 시 적절한 인덱스 URL을 사용하거나, 문서에 GPU 버전 설치 방법을 명시하는 것이 좋습니다.

---

## 발생한 문제와 해결 방법

### 문제 1: Weaviate 404 에러

**증상**: http://localhost:8080/v1/.well-known/ready 접속 시 404 에러 발생

**원인 분석**: 
처음에는 Weaviate가 제대로 시작되지 않은 것으로 생각했습니다. 그러나 docker-compose logs 명령어로 로그를 확인한 결과, Weaviate가 정상적으로 시작되었고 "Serving weaviate at http://[::]:8080"라는 메시지도 확인되었습니다.

http://localhost:8080/v1 주소로 접속해보니 API 엔드포인트 목록이 JSON으로 정상 반환되었습니다. 이것으로 Weaviate 자체는 문제가 없다는 것을 알 수 있었습니다.

PowerShell의 Invoke-WebRequest를 사용해 직접 API를 호출해본 결과, HTTP 200 상태 코드가 반환되었지만 Content-Length는 0이었습니다. 이것은 Weaviate 1.25.5 버전에서 ready 엔드포인트의 동작 방식이 변경되었기 때문입니다. 이전 버전에서는 "true"라는 텍스트를 반환했지만, 최신 버전에서는 HTTP 상태 코드만으로 준비 상태를 표현합니다.

404 에러는 실제로는 브라우저가 자동으로 요청하는 favicon.ico 파일에 대한 것이었습니다.

**해결 방법**: 
Weaviate는 정상적으로 작동하고 있었습니다. HTTP 200 상태 코드가 반환되면 Weaviate가 준비된 것으로 판단하면 됩니다. 이것은 버전별 API 변경사항으로, 문제가 아니라 정상 동작이었습니다.

**배운 점**: 
에러 메시지를 보고 즉시 문제라고 판단하지 말고, 로그를 확인하고 다른 엔드포인트도 테스트해보는 체계적인 접근이 중요합니다. 또한 소프트웨어 버전이 업데이트되면서 API 동작 방식이 변경될 수 있다는 것을 항상 염두에 두어야 합니다.

---

### 문제 2: weaviate-client와 httpx 버전 충돌

**증상**: requirements.txt에 명시된 weaviate-client 4.7.1과 httpx 0.27.2 버전이 충돌

**원인 분석**: 
pip install을 실행하려고 할 때, weaviate-client 4.7.1은 httpx 0.27.0까지만 지원한다는 에러 메시지가 나타났습니다. FastAPI 테스트를 위해서는 httpx 0.27.2가 필요한데, weaviate-client의 의존성 요구사항과 충돌하는 상황이었습니다.

웹 검색을 통해 weaviate-client의 최신 버전을 확인한 결과, 4.17.0 버전이 2024년 9월에 릴리스되었고, 이 버전은 httpx 0.26.0 이상을 지원한다는 것을 알게 되었습니다. 또한 weaviate-client 4.16.10 버전에서 httpx 호환성 문제가 수정되었다는 changelog도 발견했습니다.

**해결 방법**: 
weaviate-client를 4.17.0으로 업그레이드했습니다.

```
weaviate-client==4.17.0  # httpx 0.26.0+ 지원
```

이렇게 수정한 후 pip install을 다시 실행하니 모든 패키지가 충돌 없이 정상적으로 설치되었습니다. weaviate-client 4.17.0은 httpx 0.27.2와 완벽하게 호환되었습니다.

**배운 점**: 
패키지 버전 충돌이 발생했을 때는 먼저 각 패키지의 최신 버전과 changelog를 확인하는 것이 좋습니다. 대부분의 경우 최신 버전에서 호환성 문제가 해결되어 있습니다. 또한 이런 호환성 정보는 패키지의 공식 문서나 changelog에서 찾을 수 있습니다.

---

### 문제 3: PyTorch GPU 미인식

**증상**: torch.cuda.is_available()가 False를 반환

**원인 분석**: 
nvidia-smi 명령어로 확인한 결과 NVIDIA 드라이버는 정상적으로 설치되어 있었고, GPU도 제대로 인식되고 있었습니다. 따라서 문제는 드라이버가 아니라 PyTorch 설치 방식에 있었습니다.

기본적으로 `pip install torch`를 실행하면 CPU 전용 버전이 설치됩니다. 이것은 안전한 선택이지만, GPU를 활용하려면 명시적으로 CUDA 버전을 지정해야 합니다.

**해결 방법**: 
먼저 기존의 CPU 버전 PyTorch를 제거했습니다.

```bash
pip uninstall torch torchvision torchaudio -y
```

그리고 CUDA 12.1 버전의 PyTorch를 PyTorch 공식 저장소에서 설치했습니다.

```bash
pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

여기서 `cu121`은 CUDA 12.1을 의미합니다. 시스템의 CUDA 드라이버는 12.6이지만, CUDA는 하위 호환성을 지원하기 때문에 12.1로 빌드된 PyTorch가 12.6 드라이버에서 완벽하게 작동합니다.

재설치 후 확인해보니 torch.cuda.is_available()가 True를 반환했고, GPU 이름도 정확히 인식되었습니다.

**배운 점**: 
PyTorch의 GPU 지원은 자동으로 되는 것이 아니라 설치 시 명시적으로 CUDA 버전을 지정해야 합니다. requirements.txt에는 버전 번호만 적고, 실제 설치 시 환경에 맞는 인덱스 URL을 사용하는 것이 올바른 방법입니다. 이렇게 하면 GPU가 있는 환경에서는 CUDA 버전을, GPU가 없는 환경에서는 CPU 버전을 설치할 수 있어 유연성이 높아집니다.

---

## 최종 검증

모든 작업이 완료된 후 전체 시스템이 제대로 작동하는지 최종 검증을 진행했습니다.

### Docker 서비스 상태 확인

```bash
docker-compose ps
```

네 개의 컨테이너 모두 "Up" 상태로 정상 실행 중임을 확인했습니다.

### Python 패키지 검증

```bash
python -c "import weaviate; print('Weaviate:', weaviate.__version__)"
# 출력: Weaviate: 4.17.0

python -c "import fastapi; print('FastAPI:', fastapi.__version__)"
# 출력: FastAPI: 0.115.0

python -c "import torch; print('PyTorch:', torch.__version__)"
# 출력: PyTorch: 2.4.1+cu121
```

### GPU 인식 확인

```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0))"
# 출력:
# CUDA: True
# GPU: NVIDIA GeForce RTX 3050
```

### 웹 서비스 접근 테스트

- Weaviate API: http://localhost:8080/v1 → JSON 응답 정상
- Mongo Express: http://localhost:8081 → 웹 인터페이스 정상
- Weaviate ready check: HTTP 200 상태 코드 반환 정상

모든 검증 항목이 통과되었습니다.

---

## 완료된 환경 구성

Phase 1 완료 후 다음과 같은 개발 환경이 구축되었습니다.

### 실행 중인 서비스
- Weaviate 1.25.5: 포트 8080 (벡터 검색 준비 완료)
- MongoDB 7.0: 포트 27017 (데이터 저장 준비 완료)
- Redis 7.2: 포트 6379 (캐싱 준비 완료)
- Mongo Express 1.0: 포트 8081 (데이터베이스 관리 도구)

### Python 환경
- Python 3.11.9
- 가상환경: venv
- 설치된 주요 패키지:
  - FastAPI 0.115.0
  - PyTorch 2.4.1 (CUDA 12.1)
  - Weaviate Client 4.17.0
  - Anthropic 0.34.2
  - spaCy 3.7.6 + ko_core_news_lg
  - Sentence-Transformers 3.1.1
  - 그 외 20개 이상의 패키지

### GPU 환경
- NVIDIA Driver 560.94
- CUDA 12.6 지원
- PyTorch GPU 가속 활성화
- RTX 3050 (6GB VRAM) 사용 가능

---

## 다음 단계 (Phase 2)

Phase 2에서는 실제 데이터를 준비하고 데이터베이스에 저장하는 작업을 진행합니다.

### 준비 사항
1. Claude API 키 발급 (https://console.anthropic.com/settings/keys)
2. `.env` 파일 생성 및 API 키 설정
3. CS 팀과 데이터 수집 일정 조율

### 주요 작업
1. 제품 스펙 JSON 10개 작성
2. FAQ JSON 50개 작성 (카테고리별)
3. MongoDB 스키마 설계
4. 데이터 임포트 스크립트 실행
5. Sentence-BERT로 FAQ 임베딩 생성
6. Weaviate에 벡터 업로드

---

## 참고 명령어

### Docker 관리
```bash
# 서비스 시작
cd C:\workspace\CSAI\docker
docker-compose up -d

# 서비스 상태 확인
docker-compose ps

# 로그 확인
docker-compose logs -f weaviate

# 서비스 중지
docker-compose down
```

### Python 가상환경
```bash
# 가상환경 활성화
cd C:\workspace\CSAI\backend
venv\Scripts\activate

# 패키지 설치
pip install -r requirements.txt

# GPU 확인
python -c "import torch; print(torch.cuda.is_available())"
```

### GPU PyTorch 재설치 (필요 시)
```bash
# 기존 제거
pip uninstall torch torchvision torchaudio -y

# CUDA 버전 설치
pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

---

## 시사점 및 교훈

### 1. 체계적인 문제 해결의 중요성
이번 작업에서 여러 문제가 발생했지만, 각 문제를 체계적으로 접근하여 해결할 수 있었습니다. 로그를 확인하고, 명령어로 상태를 점검하고, 웹 검색으로 정보를 찾고, 단계별로 테스트하는 과정이 매우 효과적이었습니다.

### 2. 문서화의 가치
각 단계를 진행하면서 명령어와 결과를 기록해두니, 나중에 문제가 생겼을 때 어떤 단계에서 무엇을 했는지 정확히 추적할 수 있었습니다. 이런 기록은 비슷한 환경을 다시 구축해야 할 때나, 다른 팀원이 같은 작업을 할 때 매우 유용할 것입니다.

### 3. 버전 호환성 관리
오픈소스 패키지들은 지속적으로 업데이트되기 때문에, 버전 간 호환성 문제가 발생할 수 있습니다. 이번에 weaviate-client와 httpx의 버전 충돌을 해결한 경험은, 앞으로 다른 패키지에서 비슷한 문제가 생겼을 때 어떻게 접근해야 할지 알려줍니다.

### 4. GPU 활용의 실용성
GPU를 활성화하는 과정이 생각보다 복잡했지만, 이것은 앞으로 프로젝트 진행 속도에 큰 영향을 미칠 것입니다. CPU와 GPU의 성능 차이는 수십 배에서 수백 배까지 날 수 있기 때문에, 초기에 제대로 설정해두는 것이 중요합니다.

### 5. Docker의 장점 재확인
네 개의 서로 다른 데이터베이스를 설치하고 관리하는 것은 매우 복잡한 작업입니다. 하지만 Docker를 사용하니 docker-compose.yml 파일 하나로 모든 것을 관리할 수 있었습니다. 이것은 개발 환경의 재현성과 관리 효율성 측면에서 큰 장점입니다.

---

## 작업 시간 분석

전체 작업 시간: 약 2시간

- WSL 2 설치 및 재부팅: 20분
- Docker Desktop 설치 및 설정: 15분
- Docker Compose 서비스 실행 및 검증: 15분
- Python 환경 설정 및 패키지 설치: 30분
- GPU 환경 설정 및 문제 해결: 30분
- 최종 검증 및 문서화: 10분

---

## 결론

Phase 1 인프라 구축 작업이 성공적으로 완료되었습니다. 초기에 몇 가지 기술적인 문제가 있었지만, 모두 체계적으로 해결할 수 있었습니다. 현재 개발 환경은 다음 Phase를 진행하기에 완벽하게 준비된 상태입니다.

특히 주목할 만한 점은 이 모든 작업이 Python에 대한 사전 지식이 없는 상태에서 진행되었다는 것입니다. 각 단계마다 개념을 이해하고, 문제를 해결하는 과정을 거치면서, 실제로 작동하는 AI 개발 환경을 구축할 수 있었습니다.

이제 여러분의 컴퓨터는 전문적인 AI 개발 워크스테이션이 되었으며, Phase 2부터는 이 환경을 활용해 실제 AI 시스템을 구축하는 단계로 넘어갑니다.

---

**작성 완료**: 2025-10-01 10:55  
**다음 업데이트**: Phase 2 시작 시 (2025-10-14 예정)
