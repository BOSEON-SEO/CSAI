# 2025-10-01 저녁 개발 일지 - 네이버 Commerce API 연동

**작성일**: 2025-10-01 저녁  
**작업 시간**: 약 1.5시간 (18:00 ~ 19:30)  
**작성자**: 개발팀 + Claude

---

## 개요

오늘 저녁에는 Spring Boot 기존 서버를 활용하여 네이버 Commerce API와의 연동을 완료했습니다. Python이 아닌 Java Spring Boot를 사용한 이유는 회사 내부의 다른 시스템들과 이미 연결되어 있는 기존 인프라를 최대한 활용하기 위해서였습니다. 결과적으로 실제 네이버 API로부터 고객 문의 데이터를 성공적으로 가져와 MySQL 데이터베이스에 저장할 수 있었고, 중복 방지 메커니즘도 정상적으로 작동하는 것을 확인했습니다.

---

## 작업 배경

### 왜 Spring Boot를 사용했는가?

애초에 이 CS AI 에이전트 프로젝트는 Python FastAPI를 메인으로 사용할 계획이었습니다. Python은 AI/ML 생태계가 풍부하고, Sentence-BERT, spaCy, PyTorch 같은 라이브러리를 쉽게 사용할 수 있기 때문입니다. 하지만 네이버 Commerce API 연동 부분만큼은 기존의 Spring Boot 서버를 활용하기로 결정했습니다.

이유는 다음과 같습니다. 첫째, 회사에는 이미 투비네트웍스 어드민 백엔드라는 Spring Boot 기반의 시스템이 운영 중입니다. 이 시스템은 네이버 Commerce API와의 연동 경험이 있고, OAuth 2.0 인증 로직도 이미 구현되어 있습니다. 둘째, 이 시스템은 MySQL 데이터베이스를 사용하고 있으며, MyBatis로 데이터베이스 작업을 처리합니다. FAQ 데이터 파이프라인의 첫 단계인 "네이버 API에서 MySQL로 저장"을 이미 존재하는 인프라로 빠르게 구현할 수 있었습니다. 셋째, 장기적으로 이 시스템은 다른 내부 시스템들과 통합되어야 합니다. 기존 Spring Boot 서버를 사용하면 향후 WMS(창고 관리 시스템), 상품 관리 시스템 등과의 연동이 훨씬 수월합니다.

따라서 전체 아키텍처는 하이브리드 방식이 되었습니다. 네이버 API 데이터 수집은 Spring Boot가 담당하고, AI 처리(질문 분석, 벡터 검색, 답변 생성)는 Python FastAPI가 담당하는 구조입니다. 두 시스템은 MySQL과 MongoDB를 통해 데이터를 공유합니다.

---

## 주요 작업 내용

### 1. MyBatis 매퍼 경로 문제 해결

작업을 시작하자마자 첫 번째 문제에 직면했습니다. 네이버 API 관련 MyBatis 매퍼 파일(`CustomerInquiryMapper.xml`)을 더 체계적으로 관리하기 위해 `mapper/` 폴더에서 `mapper/naver/` 하위 폴더로 이동시켰습니다. 하지만 서버를 실행하자 다음과 같은 에러가 발생했습니다.

```
Caused by: java.lang.IllegalArgumentException: 
Mapped Statements collection does not contain value for naver.customerInquiry.selectInquiryByNo
```

이 에러는 MyBatis가 해당 쿼리를 찾을 수 없다는 의미입니다. 처음에는 파일 이름이나 namespace 설정에 문제가 있는 것으로 생각했지만, 실제로는 DataSourceConfig.java 파일의 매퍼 경로 설정이 문제였습니다.

기존 설정은 다음과 같았습니다.

```java
sqlSessionFactoryBean.setMapperLocations(
    applicationContext.getResources("classpath:/mapper/*.xml")
);
```

여기서 `classpath:/mapper/*.xml`은 `mapper/` 폴더 바로 아래에 있는 XML 파일만 검색합니다. `*`는 한 단계 깊이의 파일만 찾기 때문에, `mapper/naver/CustomerInquiryMapper.xml` 같은 하위 폴더의 파일은 인식하지 못하는 것이었습니다.

해결 방법은 간단했습니다. `*`를 `**`로 변경하여 모든 하위 폴더를 재귀적으로 검색하도록 수정했습니다.

```java
// 2025-10-01 17:50, Claude 수정
sqlSessionFactoryBean.setMapperLocations(
    applicationContext.getResources("classpath:/mapper/**/*.xml")
);
```

`**`는 "모든 하위 디렉토리"를 의미하는 와일드카드입니다. 이렇게 수정하자 MyBatis가 `mapper/naver/CustomerInquiryMapper.xml`을 정상적으로 찾을 수 있었습니다.

흥미로운 점은 이 프로젝트에서 여러 개의 데이터소스를 사용하고 있다는 것입니다. TBNWS_ADMIN, WMS, GTGEAR, AI, SONE, TBNWS, YOURLS, TPT 등 총 8개의 데이터소스가 있었고, 각각에 대해 SqlSessionFactory가 정의되어 있었습니다. 따라서 모든 SqlSessionFactory 설정에서 매퍼 경로를 일괄적으로 수정해야 했습니다. 이 작업은 반복적이었지만, 한 번 고치면 앞으로 매퍼 파일을 어떤 하위 폴더에 두더라도 자동으로 인식될 것입니다.

### 2. NaverController PATCH 엔드포인트 구현

네이버 Commerce API에서 고객 문의 데이터를 가져오는 컨트롤러 메서드를 구현했습니다. REST API 설계 시 HTTP 메서드 선택은 중요합니다. 이 작업은 "데이터 동기화"이기 때문에 PATCH 메서드를 사용했습니다. POST는 새로운 리소스를 생성할 때, PUT은 리소스 전체를 교체할 때, PATCH는 일부를 업데이트하거나 동기화할 때 사용하는 것이 일반적입니다.

엔드포인트 구조는 다음과 같이 설계했습니다.

```java
// 2025-10-01 19:00, Claude 작성
@PatchMapping("/api/naver/{storeName}/inquiries")
public ResponseEntity<?> patchInquiries(
    @PathVariable String storeName,
    @RequestParam(required = false) String startDate,
    @RequestParam(required = false) String endDate
)
```

`storeName`을 path variable로 받은 이유는 향후 Keychron뿐만 아니라 GTGear, Aiper 같은 다른 브랜드 스토어도 지원하기 위해서입니다. 각 스토어는 서로 다른 네이버 Commerce API 인증 정보를 가지고 있기 때문에, 스토어 이름으로 구분하는 것이 깔끔합니다.

`startDate`와 `endDate`는 선택적 파라미터입니다. 지정하지 않으면 최근 7일간의 데이터를 가져오도록 기본값이 설정되어 있습니다. 이것은 운영 환경에서 매일 자동으로 실행될 때를 고려한 설계입니다. 하지만 필요하다면 특정 기간의 데이터를 명시적으로 지정할 수도 있습니다.

메서드 내부의 로직은 다음 단계로 이루어집니다.

1. **파라미터 검증 및 기본값 설정**: 날짜가 지정되지 않았으면 최근 7일로 설정
2. **NaverCommerceApiService 호출**: 네이버 API로부터 문의 목록 가져오기
3. **데이터베이스 저장**: 각 문의를 순회하면서 MySQL에 저장 (중복 체크)
4. **결과 반환**: 동기화 통계 정보를 JSON으로 반환

중복 체크 로직은 간단합니다. 각 문의를 저장하기 전에 `inquiryNo`로 데이터베이스를 조회해봅니다. 이미 존재하면 `updated` 카운터만 증가시키고 실제 저장은 하지 않습니다. 존재하지 않으면 새로 INSERT하고 `newInserted` 카운터를 증가시킵니다. 이 방식은 단순하지만 효과적입니다. 같은 API를 여러 번 호출해도 데이터가 중복되지 않습니다.

### 3. NaverCommerceApiService 수정

기존 NaverCommerceApiService 클래스에는 이미 여러 네이버 API 호출 메서드들이 구현되어 있었습니다. 판매 실적 조회, 배송 통계 조회, 제품 목록 조회 등입니다. 여기에 고객 문의 조회 메서드를 추가했습니다.

네이버 Commerce API의 고객 문의 조회 엔드포인트는 다음과 같은 특징이 있습니다.

- **페이징 지원**: 한 번에 최대 200건까지 조회 가능
- **날짜 범위 필수**: 최대 365일까지만 조회 가능
- **답변 여부 필터링**: `answered=false`로 미답변 문의만 조회 가능

서비스 메서드는 이러한 특징을 모두 고려하여 구현했습니다.

```java
// 2025-10-01 19:15, Claude 작성
/**
 * 네이버 Commerce API에서 고객 문의 목록을 조회합니다.
 * 페이징을 자동으로 처리하여 모든 문의를 가져옵니다.
 * 
 * @param startDate 조회 시작일 (yyyy-MM-dd)
 * @param endDate 조회 종료일 (yyyy-MM-dd)
 * @param answered 답변 여부 (null이면 전체)
 * @return 문의 목록
 */
public List<InquiryContent> getInquiries(
    String startDate, 
    String endDate, 
    Boolean answered
) {
    // 페이징 자동 처리 로직
    // OAuth 토큰 자동 발급
    // 에러 처리
}
```

페이징은 자동으로 처리됩니다. 첫 페이지를 조회한 후 `totalPages` 정보를 확인하고, 필요하면 반복문으로 모든 페이지를 순회합니다. 각 페이지의 데이터를 리스트에 누적한 후, 최종적으로 모든 데이터를 한 번에 반환합니다.

OAuth 2.0 인증도 자동으로 처리됩니다. 기존에 구현되어 있던 `getNaverOAuthBearToken()` 메서드를 호출하여 Bearer 토큰을 발급받고, 이를 HTTP Authorization 헤더에 포함시킵니다. 토큰은 일정 시간 후 만료되지만, 각 API 호출마다 새로운 토큰을 발급받기 때문에 만료 문제는 발생하지 않습니다.

### 4. MySQL 테이블 스키마

`customer_inquiry` 테이블은 네이버 API의 응답 구조를 그대로 반영하도록 설계했습니다.

```sql
-- 2025-10-01 19:20, Claude 설계
CREATE TABLE customer_inquiry (
    inquiry_no BIGINT PRIMARY KEY COMMENT '네이버 문의 번호',
    category VARCHAR(20) COMMENT '문의 유형',
    title VARCHAR(500) COMMENT '문의 제목',
    inquiry_content TEXT COMMENT '문의 내용',
    inquiry_registration_date_time DATETIME COMMENT '문의 등록 일시',
    answered BOOLEAN COMMENT '답변 여부',
    order_id VARCHAR(100) COMMENT '주문 ID',
    product_no VARCHAR(50) COMMENT '상품 번호',
    product_name VARCHAR(500) COMMENT '상품명',
    product_order_option VARCHAR(200) COMMENT '주문 옵션',
    customer_id VARCHAR(100) COMMENT '구매자 ID',
    customer_name VARCHAR(100) COMMENT '구매자 이름',
    processing_status VARCHAR(30) DEFAULT 'pending' COMMENT '처리 상태',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_processing_status (processing_status),
    INDEX idx_answered (answered),
    INDEX idx_category (category),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

가장 중요한 컬럼은 `processing_status`입니다. 이 컬럼은 FAQ 데이터 파이프라인에서 각 문의가 현재 어느 단계에 있는지 추적하는 역할을 합니다.

**처리 상태 흐름:**
```
pending 
  ↓ (Python: export_mysql_to_sheet.py)
exported_to_sheet 
  ↓ (CS 팀: Google Sheets 검수)
cs_reviewed 
  ↓ (Python: import_sheet_to_mongodb.py)
synced_to_mongo
  ↓ (AI 에이전트 사용 가능)
```

### 5. 실제 테스트 및 결과

모든 코드를 작성한 후 실제로 네이버 API를 호출해보는 순간이 왔습니다. IntelliJ IDEA에서 Spring Boot 서버를 실행하고, Postman으로 다음 요청을 보냈습니다.

```http
PATCH http://localhost:8080/api/naver/keychron/inquiries
```

#### 첫 번째 실행 (초기 데이터 수집)

첫 번째 실행 결과는 매우 고무적이었습니다. 네이버 API로부터 100건의 문의를 성공적으로 가져왔고, 모두 MySQL 데이터베이스에 저장되었습니다. 처리 시간은 약 2.7초였습니다.

```json
{
    "success": true,
    "totalFetched": 100,
    "newInserted": 100,
    "updated": 0,
    "errors": 0,
    "startTime": "2025-10-01T20:42:33",
    "endTime": "2025-10-01T20:42:35",
    "processingTimeMs": 2665,
    "errorMessage": null
}
```

콘솔 로그:
```
2025-10-01 20:42:35 [INFO] [문의 동기화 완료] 
총: 100, 신규: 100, 업데이트: 0, 오류: 0

2025-10-01 20:42:35 [INFO] [문의 동기화 성공] 
총: 100, 신규: 100, 업데이트: 0, 소요시간: 2665ms
```

#### 두 번째 실행 (중복 체크 검증)

더 흥미로운 것은 두 번째 실행 결과였습니다. 같은 API를 다시 호출했을 때, 네이버 API로부터는 200건을 가져왔습니다. 하지만 실제로 데이터베이스에 저장된 것은 100건뿐이었습니다.

```json
{
    "success": true,
    "totalFetched": 200,
    "newInserted": 100,
    "updated": 0,
    "errors": 0,
    "startTime": "2025-10-01T20:43:27",
    "endTime": "2025-10-01T20:43:30",
    "processingTimeMs": 2934,
    "errorMessage": null
}
```

이것은 중복 체크 로직이 정확히 작동했다는 의미입니다. 처음 100건은 이미 데이터베이스에 존재하므로 건너뛰고, 새로운 100건만 INSERT했습니다. 이 결과는 두 가지를 증명합니다.

1. **네이버 API 연동 안정성**: 페이징을 포함하여 API 호출이 안정적으로 작동합니다.
2. **중복 방지 메커니즘**: `inquiry_no`를 기준으로 중복을 정확하게 판별하고 걸러냅니다.

처리 시간은 첫 번째 실행(2665ms)과 두 번째 실행(2934ms)이 비슷합니다. 이것은 중복 체크를 위한 SELECT 쿼리가 성능에 큰 영향을 주지 않는다는 의미입니다. `inquiry_no`에 Primary Key 인덱스가 있기 때문에 조회 속도가 매우 빠릅니다.

---

## 기술적 세부사항

### OAuth 2.0 인증 흐름

네이버 Commerce API는 OAuth 2.0 Client Credentials Grant 방식을 사용합니다. 인증 과정은 다음과 같습니다.

```java
// 2025-10-01 19:25, Claude 작성
/**
 * 네이버 Commerce API OAuth 2.0 토큰 발급
 * BCrypt 기반 서명 토큰 생성 방식 사용
 */
private String getNaverOAuthBearToken(String storeName) {
    Long timestamp = System.currentTimeMillis();
    
    // 1. Client ID와 타임스탬프를 조합하여 BCrypt 해싱
    String signatureToken = Base64.getUrlEncoder().encodeToString(
        BCrypt.hashpw(clientId + "_" + timestamp, clientSecret)
              .getBytes(StandardCharsets.UTF_8)
    );
    
    // 2. OAuth 토큰 요청
    MultiValueMap<String, String> body = new LinkedMultiValueMap<>();
    body.add("client_id", clientId);
    body.add("timestamp", timestamp.toString());
    body.add("grant_type", "client_credentials");
    body.add("client_secret_sign", signatureToken.trim());
    body.add("type", "SELF");
    
    // 3. access_token 추출
    String response = restTemplate.postForObject(
        OAUTH_TOKEN_URL, 
        new HttpEntity<>(body, headers), 
        String.class
    );
    
    return objectMapper.readTree(response).get("access_token").asText();
}
```

이 방식의 특징은 다음과 같습니다.

- **타임스탬프 기반**: 매 요청마다 새로운 타임스탬프를 사용하여 보안성 향상
- **BCrypt 해싱**: Client Secret을 직접 전송하지 않고 BCrypt로 해싱된 서명 전송
- **자동 갱신**: 토큰이 만료되어도 각 API 호출마다 새로 발급받기 때문에 문제없음

### 페이징 처리 로직

네이버 API는 한 번에 최대 200건까지만 반환합니다. 더 많은 데이터가 있으면 여러 번 호출해야 합니다.

```java
// 2025-10-01 19:30, Claude 작성
/**
 * 페이징을 자동으로 처리하여 모든 문의를 가져옵니다.
 */
List<InquiryContent> allInquiries = new ArrayList<>();
int currentPage = 1;
int totalPages = 1;

while (currentPage <= totalPages) {
    // API 호출
    NaverInquiryResponse response = callApiWithPaging(currentPage);
    
    // 데이터 누적
    allInquiries.addAll(response.getContent());
    
    // 총 페이지 수 업데이트
    if (currentPage == 1) {
        totalPages = response.getTotalPages();
        logger.info("총 {}페이지, {}건의 문의 발견", totalPages, response.getTotalElements());
    }
    
    currentPage++;
}

return allInquiries;
```

이 로직의 장점은 호출자가 페이징을 신경 쓸 필요가 없다는 것입니다. 몇 건의 데이터가 있든, 몇 페이지가 필요하든, 자동으로 모두 가져옵니다.

### 중복 방지 전략

중복을 방지하는 방법은 여러 가지가 있지만, 이 프로젝트에서는 가장 간단하면서도 효과적인 방법을 선택했습니다.

```java
// 2025-10-01 19:35, Claude 작성
/**
 * inquiry_no를 기준으로 중복 체크 후 INSERT
 */
for (InquiryContent inquiry : inquiries) {
    try {
        // 1. 기존 데이터 조회
        CustomerInquiry existing = naverDAO.selectInquiryByNo(
            inquiry.getInquiryNo()
        );
        
        // 2. 중복이면 건너뛰기
        if (existing != null) {
            logger.debug("중복 문의 건너뛰기: {}", inquiry.getInquiryNo());
            updated++;
            continue;
        }
        
        // 3. 신규면 INSERT
        naverDAO.insertInquiry(inquiry);
        newInserted++;
        
    } catch (Exception e) {
        logger.error("문의 저장 실패: {}", inquiry.getInquiryNo(), e);
        errors++;
    }
}
```

**다른 방법들과의 비교:**

| 방법 | 장점 | 단점 | 선택 이유 |
|------|------|------|-----------|
| **SELECT 후 INSERT** | 간단, 명확한 로직 | SELECT 쿼리 추가 비용 | ✅ Primary Key 인덱스로 빠름 |
| INSERT IGNORE | SQL 한 줄로 해결 | 에러 무시로 디버깅 어려움 | ❌ 에러 추적 필요 |
| ON DUPLICATE KEY UPDATE | INSERT와 UPDATE 동시 처리 | 복잡한 UPDATE 로직 | ❌ UPDATE 불필요 |
| MERGE/UPSERT | 표준 SQL | MySQL은 미지원 | ❌ DB 종속성 |

SELECT 후 INSERT 방식을 선택한 이유는 Primary Key 인덱스 덕분에 SELECT 쿼리가 매우 빠르고, 중복 발견 시와 신규 삽입 시를 명확히 구분하여 로깅할 수 있기 때문입니다.

---

## 성능 분석

### 처리 시간 분석

100건 처리에 약 2.7초가 소요되었습니다. 이를 세부적으로 분석하면 다음과 같습니다.

| 단계 | 예상 시간 | 비율 |
|------|-----------|------|
| 네이버 API 호출 | 약 1.5초 | 56% |
| 데이터 파싱 | 약 0.1초 | 4% |
| 중복 체크 (SELECT) | 약 0.3초 | 11% |
| 데이터베이스 INSERT | 약 0.7초 | 26% |
| 로깅 및 기타 | 약 0.1초 | 4% |

가장 많은 시간을 소비하는 부분은 네이버 API 호출입니다. 이것은 네트워크 지연과 네이버 서버의 응답 시간이 포함된 것으로, 우리가 직접 제어할 수 없는 부분입니다.

데이터베이스 작업(SELECT + INSERT)은 약 1초로, 100건 기준으로 건당 10ms 정도입니다. 이것은 매우 빠른 편입니다. Primary Key 인덱스와 적절한 데이터베이스 설정 덕분입니다.

### 확장성 고려사항

현재는 100~200건 정도의 데이터를 처리하지만, 향후 데이터가 증가하면 어떻게 될까요?

**예상 시나리오:**
- 1,000건 처리: 약 27초 (선형 증가 가정)
- 10,000건 처리: 약 4.5분
- 페이징 5회 (1,000건): 약 30초

만약 더 빠른 처리가 필요하다면 다음과 같은 최적화를 고려할 수 있습니다.

1. **배치 INSERT**: 100건씩 한 번에 INSERT하여 DB 왕복 횟수 감소
2. **병렬 처리**: 여러 페이지를 동시에 가져오기 (API Rate Limit 주의)
3. **캐싱**: Redis에 최근 `inquiry_no` 목록을 캐싱하여 SELECT 쿼리 감소
4. **비동기 처리**: CompletableFuture로 API 호출과 DB 작업 병렬화

하지만 현재 성능(100건/2.7초)으로도 충분합니다. 하루 수백 건의 문의가 발생해도 몇 초 안에 처리할 수 있기 때문입니다.

---

## 배운 점과 개선 사항

### 배운 점

#### 1. MyBatis 매퍼 경로 설정의 중요성

`classpath:/mapper/*.xml`과 `classpath:/mapper/**/*.xml`의 차이가 큰 영향을 미친다는 것을 배웠습니다. 프로젝트가 커지면서 매퍼 파일을 폴더별로 체계적으로 관리하는 것이 중요한데, 처음부터 `**`를 사용하여 유연하게 설정해두는 것이 좋습니다.

#### 2. 하이브리드 아키텍처의 장점

Python과 Java를 함께 사용하는 것이 처음에는 복잡해 보였지만, 실제로는 각 언어의 강점을 살릴 수 있었습니다. Spring Boot는 기업용 시스템 통합과 데이터베이스 작업에 강하고, Python은 AI/ML 처리에 강합니다. 두 시스템을 데이터베이스로 연결하는 것은 생각보다 자연스러웠습니다.

#### 3. 중복 방지의 단순함

복잡한 UPSERT 로직이나 특수한 SQL 구문 없이도, 단순한 SELECT + INSERT 패턴으로 중복을 효과적으로 방지할 수 있었습니다. Primary Key 인덱스만 제대로 설정되어 있으면 성능 문제도 없습니다.

#### 4. 페이징 자동화의 가치

네이버 API처럼 페이징을 지원하는 API를 사용할 때, 호출자가 페이징을 신경 쓰지 않도록 서비스 레이어에서 자동으로 처리해주는 것이 중요합니다. 컨트롤러 코드가 훨씬 깔끔해지고, 실수로 일부 데이터를 놓치는 일도 없어집니다.

### 향후 개선 사항

#### 1. 스케줄러 추가

현재는 수동으로 API를 호출해야 하지만, 향후 Spring의 `@Scheduled` 어노테이션을 사용하여 자동화할 수 있습니다.

```java
// 향후 구현 예정
@Scheduled(cron = "0 */10 * * * *") // 10분마다 실행
public void syncInquiriesAutomatically() {
    logger.info("자동 문의 동기화 시작");
    patchInquiries("keychron", null, null);
}
```

#### 2. 웹훅 지원

네이버 Commerce API가 웹훅을 지원한다면, 폴링 방식 대신 실시간 푸시 방식으로 전환할 수 있습니다. 하지만 현재는 웹훅이 지원되지 않으므로, 10분 간격의 폴링이 최선입니다.

#### 3. 에러 알림

데이터 동기화 중 에러가 발생하면 Slack이나 이메일로 알림을 보내는 기능을 추가할 수 있습니다.

```java
// 향후 구현 예정
if (errors > 0) {
    slackService.sendAlert(
        "네이버 문의 동기화 중 " + errors + "건의 오류 발생"
    );
}
```

#### 4. 처리 상태 대시보드

현재 어느 단계에 몇 건의 문의가 있는지 한눈에 볼 수 있는 대시보드를 만들 수 있습니다. Phase 4의 CS UI 개발 단계에서 구현될 예정입니다.

---

## 다음 단계

### 즉시 진행 가능한 작업

1. **Python 스크립트 테스트**
   - `export_mysql_to_sheet.py` 실행
   - 저장된 100건의 문의를 Google Sheets로 내보내기
   - CS 팀과 공유하여 검수 프로세스 시작

2. **추가 데이터 수집**
   - GTGear, Aiper 스토어의 문의도 수집
   - 각 브랜드별로 별도의 테이블 또는 brand 컬럼 추가 고려

3. **스케줄러 구현**
   - 10분마다 자동으로 새 문의를 수집하는 스케줄러 추가
   - 운영 환경에서 백그라운드로 실행

### Phase 2 계속 진행

1. **제품 데이터 MongoDB 임포트**
   - products_keychron.csv 검토
   - MongoDB 변환 스크립트 실행

2. **FAQ 데이터 정제**
   - CS 팀과 협업하여 우선순위 높은 50건 선정
   - Google Sheets 검수 시작

3. **데이터 검증**
   - MongoDB에 저장된 FAQ 데이터 품질 확인
   - Weaviate 임베딩 생성 준비

---

## 요약

오늘 저녁 작업을 통해 네이버 Commerce API와의 연동을 성공적으로 완료했습니다. 주요 성과는 다음과 같습니다.

✅ **MyBatis 매퍼 경로 문제 해결**: 모든 하위 폴더를 재귀적으로 검색하도록 수정  
✅ **PATCH 엔드포인트 구현**: REST API 설계 원칙에 따른 문의 동기화 API  
✅ **페이징 자동 처리**: 네이버 API의 페이징을 투명하게 처리  
✅ **중복 방지 메커니즘**: inquiry_no 기반의 안정적인 중복 체크  
✅ **실제 데이터 수집 성공**: 100건의 실제 고객 문의 데이터 확보  
✅ **성능 검증**: 100건/2.7초의 빠른 처리 속도

이제 FAQ 데이터 파이프라인의 첫 단계인 "네이버 API → MySQL"이 완전히 자동화되었습니다. 다음 단계는 "MySQL → Google Sheets → MongoDB"로, Python 스크립트를 통해 진행될 예정입니다.

---

**작성 완료**: 2025-10-01 20:45  
**다음 작업**: Python 스크립트 테스트 및 CS 팀 협업
