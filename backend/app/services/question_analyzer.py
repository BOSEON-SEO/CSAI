#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì§ˆë¬¸ ë¶„ì„ ì„œë¹„ìŠ¤ (Question Analyzer)
íˆ¬ë¹„ë„¤íŠ¸ì›ìŠ¤ ê¸€ë¡œë²Œ - CS AI ì—ì´ì „íŠ¸ í”„ë¡œì íŠ¸

2025-10-02 09:15, Claude ì‘ì„±
2025-10-02 16:00, Claude ì—…ë°ì´íŠ¸ (hybrid_search íŒŒë¼ë¯¸í„° ìˆ˜ì •)

ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ì„í•˜ì—¬:
1. í‚¤ì›Œë“œ ì¶”ì¶œ (spaCy)
2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
3. ì œí’ˆ ì½”ë“œ ì¸ì‹
4. ì„ë² ë”© ìƒì„± (Sentence-BERT)
5. ìœ ì‚¬ FAQ ê²€ìƒ‰ (Weaviate)
6. ë³µì¡ë„ íŒë‹¨
"""

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field

import spacy
from sentence_transformers import SentenceTransformer
import torch

from .weaviate_service import WeaviateService


# ==================== ë¡œê¹… ì„¤ì • ====================

logger = logging.getLogger(__name__)


# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================

@dataclass
class AnalysisResult:
    """
    ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
    
    Attributes:
        keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        product_codes: ì¸ì‹ëœ ì œí’ˆ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['K10', 'PRO MAX'])
        category: ì¶”ì • ì¹´í…Œê³ ë¦¬ (ë°°ì†¡/ë°˜í’ˆ/ìƒí’ˆ/êµí™˜/í™˜ë¶ˆ/ê¸°íƒ€)
        complexity_score: ë³µì¡ë„ ì ìˆ˜ (0.0 ~ 1.0)
        embedding: ì§ˆë¬¸ ì„ë² ë”© ë²¡í„° (768ì°¨ì›)
        similar_faqs: ìœ ì‚¬ FAQ ë¦¬ìŠ¤íŠ¸
        confidence: ë‹µë³€ ê°€ëŠ¥ ì‹ ë¢°ë„ (0.0 ~ 1.0)
        should_defer: ì‚¬ëŒì—ê²Œ ì „ê°€ ì—¬ë¶€
        defer_reason: ì „ê°€ ì‚¬ìœ 
    """
    keywords: List[str] = field(default_factory=list)
    product_codes: List[str] = field(default_factory=list)
    category: str = "ê¸°íƒ€"
    complexity_score: float = 0.0
    embedding: Optional[List[float]] = None
    similar_faqs: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0
    should_defer: bool = False
    defer_reason: Optional[str] = None


# ==================== ì§ˆë¬¸ ë¶„ì„ê¸° ====================

class QuestionAnalyzer:
    """
    ì§ˆë¬¸ ë¶„ì„ ì„œë¹„ìŠ¤
    
    spaCy + Sentence-BERT + Weaviateë¥¼ í™œìš©í•˜ì—¬
    ê³ ê° ë¬¸ì˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ (í™•ì¥ ê°€ëŠ¥)
    CATEGORY_KEYWORDS = {
        'ë°°ì†¡': ['ë°°ì†¡', 'ë„ì°©', 'ë°œì†¡', 'íƒë°°', 'ì†¡ì¥', 'ìˆ˜ë ¹', 'ë°›', 'ì–¸ì œ'],
        'ë°˜í’ˆ': ['ë°˜í’ˆ', 'í™˜ë¶ˆ', 'ì·¨ì†Œ', 'ë°˜ì†¡', 'ìˆ˜ê±°'],
        'êµí™˜': ['êµí™˜', 'ë³€ê²½', 'ë°”ê¾¸', 'ë‹¤ë¥¸'],
        'ìƒí’ˆ': ['ë¶ˆëŸ‰', 'ê³ ì¥', 'ì‘ë™', 'ì¸ì‹', 'ì—°ê²°', 'ì•ˆë¨', 'ì•ˆë¼', 'ë¬¸ì œ'],
        'í™˜ë¶ˆ': ['í™˜ë¶ˆ', 'ëˆ', 'ê²°ì œ', 'ì·¨ì†Œ'],
        'ê¸°íƒ€': ['ë¬¸ì˜', 'ì§ˆë¬¸', 'ê¶ê¸ˆ']
    }
    
    # ì œí’ˆ ì½”ë“œ íŒ¨í„´ (ì •ê·œì‹)
    PRODUCT_CODE_PATTERNS = [
        r'K\d{1,2}',  # K10, K8, K5 ë“±
        r'Q\d{1,2}',  # Q10, Q13 ë“±
        r'V\d{1,2}',  # V10, V6 ë“±
        r'B\d{1,2}',  # B6, B1 ë“±
        r'C\d{1,2}',  # C2 ë“±
        r'M\d{1,2}',  # M6 (ë§ˆìš°ìŠ¤)
        r'PRO\s*MAX',
        r'PRO\s*SE\d?',
        r'ZMK',
    ]
    
    # ë³µì¡ë„ íŒë‹¨ í‚¤ì›Œë“œ
    HIGH_COMPLEXITY_KEYWORDS = [
        'íŒì›¨ì–´', 'firmware', 'ë“œë¼ì´ë²„', 'driver',
        'í˜¸í™˜', 'ì§€ì›', 'bios', 'ë°”ì´ì˜¤ìŠ¤',
        'ì—…ë°ì´íŠ¸', 'update', 'ë²„ì „', 'version',
        'ë¸”ë£¨íˆ¬ìŠ¤', 'bluetooth', 'ì—°ê²°', 'ëŠê¹€',
        'ì¸ì‹', 'í˜ì–´ë§', 'pairing', 'ë¬´ì„ '
    ]
    
    def __init__(
        self,
        spacy_model: str = "ko_core_news_sm",
        sbert_model: str = "jhgan/ko-sroberta-multitask",
        weaviate_service: Optional[WeaviateService] = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            spacy_model: spaCy í•œêµ­ì–´ ëª¨ë¸ ì´ë¦„
            sbert_model: Sentence-BERT ëª¨ë¸ ì´ë¦„
            weaviate_service: WeaviateService ì¸ìŠ¤í„´ìŠ¤
        """
        logger.info("ğŸ¤– QuestionAnalyzer ì´ˆê¸°í™” ì¤‘...")
        
        # spaCy ëª¨ë¸ ë¡œë“œ
        logger.info(f"  ğŸ“š spaCy ëª¨ë¸ ë¡œë”©: {spacy_model}")
        try:
            self.nlp = spacy.load(spacy_model)
        except OSError:
            logger.error(f"  âŒ spaCy ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {spacy_model}")
            logger.info(f"  ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹: python -m spacy download {spacy_model}")
            raise
        
        # Sentence-BERT ëª¨ë¸ ë¡œë“œ
        logger.info(f"  ğŸ§  Sentence-BERT ëª¨ë¸ ë¡œë”©: {sbert_model}")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"     ë””ë°”ì´ìŠ¤: {device}")
        self.sbert = SentenceTransformer(sbert_model, device=device)
        
        # Weaviate ì„œë¹„ìŠ¤
        self.weaviate = weaviate_service
        
        logger.info("  âœ… QuestionAnalyzer ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (spaCy)
        
        ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, ë™ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            top_k: ìƒìœ„ ëª‡ ê°œ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í• ì§€
        
        Returns:
            í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        doc = self.nlp(text)
        
        # í’ˆì‚¬ í•„í„°ë§: ëª…ì‚¬(NOUN), ê³ ìœ ëª…ì‚¬(PROPN), ë™ì‚¬(VERB)
        keywords = []
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN', 'VERB'] and len(token.text) > 1:
                keywords.append(token.text)
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
        keyword_freq = {}
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_keywords = sorted(
            keyword_freq.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # ìƒìœ„ Kê°œ ë°˜í™˜
        return [keyword for keyword, _ in sorted_keywords[:top_k]]
    
    def extract_product_codes(self, text: str) -> List[str]:
        """
        ì œí’ˆ ì½”ë“œ ì¶”ì¶œ (ì •ê·œì‹)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì œí’ˆ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        """
        codes = []
        
        for pattern in self.PRODUCT_CODE_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            codes.extend([m.upper().strip() for m in matches])
        
        # ì¤‘ë³µ ì œê±°
        return list(set(codes))
    
    def classify_category(self, text: str, keywords: List[str]) -> str:
        """
        ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        
        í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ
        
        Returns:
            ì¹´í…Œê³ ë¦¬ (ë°°ì†¡/ë°˜í’ˆ/êµí™˜/ìƒí’ˆ/í™˜ë¶ˆ/ê¸°íƒ€)
        """
        text_lower = text.lower()
        keywords_lower = [k.lower() for k in keywords]
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for category, category_keywords in self.CATEGORY_KEYWORDS.items():
            score = 0
            for keyword in category_keywords:
                # í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ë°œê²¬
                if keyword in text_lower:
                    score += 2
                
                # ì¶”ì¶œëœ í‚¤ì›Œë“œì— í¬í•¨
                if keyword in keywords_lower:
                    score += 1
            
            scores[category] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        else:
            return "ê¸°íƒ€"
    
    def calculate_complexity(self, text: str, keywords: List[str]) -> float:
        """
        ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        
        ë‹¤ìŒ ìš”ì†Œë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤:
        - ê³ ë³µì¡ë„ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        - í…ìŠ¤íŠ¸ ê¸¸ì´
        - ì§ˆë¬¸ ë¬¸ì¥ ìˆ˜
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ
        
        Returns:
            ë³µì¡ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        score = 0.0
        
        text_lower = text.lower()
        
        # 1. ê³ ë³µì¡ë„ í‚¤ì›Œë“œ ì²´í¬ (ê°€ì¤‘ì¹˜: 0.5)
        complexity_keyword_count = 0
        for keyword in self.HIGH_COMPLEXITY_KEYWORDS:
            if keyword in text_lower:
                complexity_keyword_count += 1
        
        if complexity_keyword_count > 0:
            score += 0.5
        
        # 2. í…ìŠ¤íŠ¸ ê¸¸ì´ (ê°€ì¤‘ì¹˜: 0.3)
        if len(text) > 200:
            score += 0.3
        elif len(text) > 100:
            score += 0.15
        
        # 3. ì§ˆë¬¸ ë¬¸ì¥ ìˆ˜ (ê°€ì¤‘ì¹˜: 0.2)
        question_marks = text.count('?') + text.count('?')
        if question_marks >= 3:
            score += 0.2
        elif question_marks >= 2:
            score += 0.1
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Sentence-BERT)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        
        Returns:
            768ì°¨ì› ì„ë² ë”© ë²¡í„°
        """
        embedding = self.sbert.encode(
            text,
            convert_to_tensor=False,
            normalize_embeddings=True
        )
        return embedding.tolist()
    
    def calculate_confidence(
        self,
        similar_faqs: List[Dict[str, Any]],
        complexity_score: float
    ) -> Tuple[float, bool, Optional[str]]:
        """
        ë‹µë³€ ê°€ëŠ¥ ì‹ ë¢°ë„ ê³„ì‚°
        
        Args:
            similar_faqs: ìœ ì‚¬ FAQ ë¦¬ìŠ¤íŠ¸
            complexity_score: ë³µì¡ë„ ì ìˆ˜
        
        Returns:
            (ì‹ ë¢°ë„, ì „ê°€ ì—¬ë¶€, ì „ê°€ ì‚¬ìœ )
        """
        # ê¸°ë³¸ ì‹ ë¢°ë„: ìœ ì‚¬ FAQì˜ í‰ê·  ì ìˆ˜/ìœ ì‚¬ë„
        if not similar_faqs:
            return 0.0, True, "ìœ ì‚¬í•œ FAQë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        # 'score' (í•˜ì´ë¸Œë¦¬ë“œ) ë˜ëŠ” 'similarity' (ë²¡í„°ë§Œ) ì‚¬ìš©
        avg_similarity = sum(
            faq.get('score', faq.get('similarity', 0)) 
            for faq in similar_faqs
        ) / len(similar_faqs)
        
        # ë³µì¡ë„ íŒ¨ë„í‹°
        confidence = avg_similarity * (1 - complexity_score * 0.3)
        
        # ì „ê°€ íŒë‹¨
        should_defer = False
        defer_reason = None
        
        # ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ ì „ê°€
        if complexity_score > 0.7:
            should_defer = True
            defer_reason = "ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•œ ë¬¸ì˜ì…ë‹ˆë‹¤ (íŒì›¨ì–´/ë“œë¼ì´ë²„/í˜¸í™˜ì„± ë“±)"
        
        # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ì „ê°€
        elif avg_similarity < 0.6:
            should_defer = True
            defer_reason = "ìœ ì‚¬í•œ ì´ì „ ì‚¬ë¡€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"
        
        # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì „ê°€
        elif confidence < 0.5:
            should_defer = True
            defer_reason = "ë‹µë³€ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤"
        
        return confidence, should_defer, defer_reason
    
    async def analyze(
        self,
        inquiry_content: str,
        brand_channel: str,
        title: Optional[str] = None,
        category: Optional[str] = None,
        product_name: Optional[str] = None
    ) -> AnalysisResult:
        """
        ì§ˆë¬¸ ì¢…í•© ë¶„ì„
        
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸:
        1. í‚¤ì›Œë“œ ì¶”ì¶œ
        2. ì œí’ˆ ì½”ë“œ ì¸ì‹
        3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        4. ë³µì¡ë„ ê³„ì‚°
        5. ì„ë² ë”© ìƒì„±
        6. ìœ ì‚¬ FAQ ê²€ìƒ‰
        7. ì‹ ë¢°ë„ í‰ê°€
        
        Args:
            inquiry_content: ë¬¸ì˜ ë‚´ìš©
            brand_channel: ë¸Œëœë“œ ì±„ë„
            title: ë¬¸ì˜ ì œëª© (ì„ íƒ)
            category: ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ (ì„ íƒ, ì—†ìœ¼ë©´ ìë™ ë¶„ë¥˜)
            product_name: ì œí’ˆëª… (ì„ íƒ)
        
        Returns:
            AnalysisResult ê°ì²´
        """
        logger.info(f"ğŸ“ ì§ˆë¬¸ ë¶„ì„ ì‹œì‘: '{inquiry_content[:50]}...'")
        
        result = AnalysisResult()
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (ì œëª© + ë‚´ìš©)
        full_text = f"{title or ''} {inquiry_content} {product_name or ''}".strip()
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        logger.info("  ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        result.keywords = self.extract_keywords(full_text)
        logger.info(f"     í‚¤ì›Œë“œ: {result.keywords[:5]}")
        
        # 2. ì œí’ˆ ì½”ë“œ ì¸ì‹
        logger.info("  ğŸ·ï¸  ì œí’ˆ ì½”ë“œ ì¸ì‹ ì¤‘...")
        result.product_codes = self.extract_product_codes(full_text)
        if result.product_codes:
            logger.info(f"     ì œí’ˆ ì½”ë“œ: {result.product_codes}")
        
        # 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        if category:
            result.category = category
        else:
            logger.info("  ğŸ“‚ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")
            result.category = self.classify_category(full_text, result.keywords)
            logger.info(f"     ì¹´í…Œê³ ë¦¬: {result.category}")
        
        # 4. ë³µì¡ë„ ê³„ì‚°
        logger.info("  ğŸ§® ë³µì¡ë„ ê³„ì‚° ì¤‘...")
        result.complexity_score = self.calculate_complexity(full_text, result.keywords)
        logger.info(f"     ë³µì¡ë„: {result.complexity_score:.2f}")
        
        # 5. ì„ë² ë”© ìƒì„±
        logger.info("  ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘...")
        result.embedding = self.generate_embedding(inquiry_content)
        logger.info(f"     ì„ë² ë”©: 768ì°¨ì› ë²¡í„°")
        
        # 6. ìœ ì‚¬ FAQ ê²€ìƒ‰ (Weaviate)
        if self.weaviate:
            logger.info("  ğŸ” ìœ ì‚¬ FAQ ê²€ìƒ‰ ì¤‘...")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)
            result.similar_faqs = await self.weaviate.hybrid_search(
                query_text=inquiry_content,
                keywords=result.keywords[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
                brand_channel=brand_channel,
                category=result.category if result.category != "ê¸°íƒ€" else None,
                limit=5
            )
            
            # ìµœì†Œ ì ìˆ˜ í•„í„°ë§ (0.5 ì´ìƒë§Œ)
            result.similar_faqs = [
                faq for faq in result.similar_faqs 
                if faq.get('score', 0) >= 0.5
            ]
            
            logger.info(f"     ìœ ì‚¬ FAQ: {len(result.similar_faqs)}ê°œ ë°œê²¬")
            
            if result.similar_faqs:
                top_score = result.similar_faqs[0].get('score', 0)
                logger.info(f"     ìµœê³  ì ìˆ˜: {top_score:.2f}")
        
        # 7. ì‹ ë¢°ë„ í‰ê°€
        logger.info("  ğŸ“Š ì‹ ë¢°ë„ í‰ê°€ ì¤‘...")
        result.confidence, result.should_defer, result.defer_reason = \
            self.calculate_confidence(result.similar_faqs, result.complexity_score)
        
        logger.info(f"     ì‹ ë¢°ë„: {result.confidence:.2f}")
        logger.info(f"     ì „ê°€ ì—¬ë¶€: {result.should_defer}")
        if result.defer_reason:
            logger.info(f"     ì „ê°€ ì‚¬ìœ : {result.defer_reason}")
        
        logger.info("  âœ… ë¶„ì„ ì™„ë£Œ!\n")
        
        return result


# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def format_analysis_result(result: AnalysisResult) -> str:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
    
    Args:
        result: AnalysisResult ê°ì²´
    
    Returns:
        í¬ë§·íŒ…ëœ ë¬¸ìì—´
    """
    lines = [
        "=" * 70,
        "ğŸ“Š ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼",
        "=" * 70,
        "",
        f"ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(result.keywords[:10])}",
        f"ğŸ·ï¸  ì œí’ˆ ì½”ë“œ: {', '.join(result.product_codes) if result.product_codes else 'N/A'}",
        f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {result.category}",
        f"ğŸ§® ë³µì¡ë„: {result.complexity_score:.2f}",
        "",
        f"ğŸ” ìœ ì‚¬ FAQ: {len(result.similar_faqs)}ê°œ ë°œê²¬",
    ]
    
    # ìœ ì‚¬ FAQ ìƒìœ„ 3ê°œ
    if result.similar_faqs:
        lines.append("")
        for i, faq in enumerate(result.similar_faqs[:3], 1):
            score = faq.get('score', faq.get('similarity', 0))
            lines.append(f"  [{i}] ì ìˆ˜ {score:.2f}: {faq.get('title', 'N/A')}")
    
    lines.extend([
        "",
        f"ğŸ“Š ì‹ ë¢°ë„: {result.confidence:.2f}",
        f"âš ï¸  ì „ê°€ ì—¬ë¶€: {'ì˜ˆ' if result.should_defer else 'ì•„ë‹ˆì˜¤'}",
    ])
    
    if result.defer_reason:
        lines.append(f"   ì‚¬ìœ : {result.defer_reason}")
    
    lines.append("=" * 70)
    
    return '\n'.join(lines)
